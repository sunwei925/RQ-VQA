{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8397a614-fbe4-4717-93c2-0eda070c86d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "\n",
    "from q_align.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from q_align.conversation import conv_templates, SeparatorStyle\n",
    "from q_align.model.builder import load_pretrained_model\n",
    "from q_align.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import TextStreamer\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def disable_torch_init():\n",
    "    \"\"\"\n",
    "    Disable the redundant torch default initialization to accelerate model creation.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    setattr(torch.nn.Linear, \"reset_parameters\", lambda self: None)\n",
    "    setattr(torch.nn.LayerNorm, \"reset_parameters\", lambda self: None)\n",
    "\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith('http://') or image_file.startswith('https://'):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51dea12f-3b7f-472d-9b7b-104bc58ad771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Instantiating LlamaAttention without passing `layer_idx` is not recommended and will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` when creating this class.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d1b26546f44ec199d83feb2f42ecaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "disable_torch_init()\n",
    "\n",
    "model_name = get_model_name_from_path(\"q-future/one-align\")\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\"q-future/one-align\", None, model_name, True, True, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f8dc6f-446f-40a8-addc-52034af65a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def expand2square(pil_img, background_color):\n",
    "    width, height = pil_img.size\n",
    "    if width == height:\n",
    "        return pil_img\n",
    "    elif width > height:\n",
    "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "        result.paste(pil_img, (0, (width - height) // 2))\n",
    "        return result\n",
    "    else:\n",
    "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "        result.paste(pil_img, ((height - width) // 2, 0))\n",
    "        return result\n",
    "\n",
    "def get_image_tensor(image):\n",
    "    image = expand2square(image, tuple(int(x*255) for x in image_processor.image_mean))\n",
    "    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].half().to(\"cuda:0\")\n",
    "    return image_tensor\n",
    "\n",
    "def extract_frames(video_path):\n",
    "    # 打开视频文件\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # 获取视频的FPS（每秒帧数）\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # 初始化一个计数器用于计算间隔\n",
    "    count = 0\n",
    "    up_tensors = []\n",
    "    low_tensors = []\n",
    "    # 读取视频直到结束\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # 如果正确读取帧，则ret为True\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # 每秒抽取一帧\n",
    "        if count % fps == 0:\n",
    "            # 将帧转换为PIL图像格式\n",
    "            pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            # 计算图像的上半部分和下半部分\n",
    "            width, height = pil_image.size\n",
    "            upper_half = pil_image.crop((0, 0, width, height // 2))\n",
    "            lower_half = pil_image.crop((0, height // 2, width, height))\n",
    "            upper_half = get_image_tensor(upper_half)\n",
    "            lower_half = get_image_tensor(lower_half)\n",
    "            up_tensors.append(upper_half)\n",
    "            low_tensors.append(lower_half)\n",
    "            # 这里可以根据需要处理或显示图像的上半部分和下半部分\n",
    "            # 例如，显示上半部分和下半部分\n",
    "            # upper_half.show()\n",
    "            # lower_half.show()\n",
    "        \n",
    "        count += 1\n",
    "    \n",
    "    # 释放和关闭视频文件\n",
    "    cap.release()\n",
    "    return up_tensors,low_tensors\n",
    "\n",
    "# 调用函数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e939b8e-a53c-41d9-983b-5f5bb3f77572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'poor', 'high', 'fair', 'low', 'excellent', 'bad', 'fine', 'moderate', 'decent', 'average', 'medium', 'acceptable']\n",
      "[1781, 6460, 1880, 6534, 4482, 15129, 4319, 2691, 17768, 27189, 6588, 18350, 22691]\n",
      "torch.Size([2, 2, 4096])\n",
      "(2, 4096)\n"
     ]
    }
   ],
   "source": [
    "conv_mode = \"mplug_owl2\"   \n",
    "inp = \"How would you rate the quality of this image?\"   \n",
    "conv = conv_templates[conv_mode].copy()\n",
    "inp =  inp + \"\\n\" + DEFAULT_IMAGE_TOKEN\n",
    "conv.append_message(conv.roles[0], inp)\n",
    "image = None\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt() + \" The quality of the image is\"\n",
    "toks = [\"good\", \"poor\", \"high\", \"fair\", \"low\", \"excellent\", \"bad\", \"fine\", \"moderate\",  \"decent\", \"average\", \"medium\", \"acceptable\"]\n",
    "print(toks)\n",
    "ids_ = [id_[1] for id_ in tokenizer(toks)[\"input_ids\"]]\n",
    "print(ids_)\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(\"cuda:0\")\n",
    "\n",
    "def get_hidden_states(video_path = '/workspace/val/val/0009.mp4', save_path = '/workspace/test.npy'):\n",
    "    video_path = video_path  # 请替换为你的视频路径\n",
    "    up_tensors,low_tensors = extract_frames(video_path)\n",
    "    \n",
    "    #请进入Q-Align/q_align/model/modeling_mplug_owl2.py将forward函数里的(347行)return CausalLMOutputWithPast里的logits修改为hidden_states传出\n",
    "                \n",
    "    with torch.inference_mode():\n",
    "        up_hidden_states = model(input_ids.repeat(len(up_tensors), 1),\n",
    "            images=torch.cat(up_tensors, 0))['logits']\n",
    "    with torch.inference_mode():\n",
    "        low_hidden_states = model(input_ids.repeat(len(low_tensors), 1),\n",
    "            images=torch.cat(low_tensors, 0))['logits']\n",
    "        \n",
    "    stacked_features = torch.stack((torch.mean(up_hidden_states, dim=1, keepdim=False), torch.mean(low_hidden_states, dim=1, keepdim=False)), dim=1)\n",
    "    print(stacked_features.shape)\n",
    "    # 沿着堆叠的维度（即第0维）计算平均值，得到最终的平均特征张量，形状为[2, 4096]\n",
    "    average_features = torch.mean(stacked_features, dim=1)\n",
    "    \n",
    "    average_features_cpu = average_features.cpu()\n",
    "    \n",
    "    # 将PyTorch张量转换为NumPy数组\n",
    "    average_features_numpy = average_features_cpu.numpy()\n",
    "    print(average_features_numpy.shape)\n",
    "    # 存储NumPy数组到文件，这里使用.npy格式\n",
    "    np.save(save_path, average_features_numpy)\n",
    "\n",
    "#get_hidden_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9479030a-8db0-4bb9-8d9b-e5d29ef0aa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/val/val/0420.mp4\n",
      "/workspace/qalign_features/val/0420.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0419.mp4\n",
      "/workspace/qalign_features/val/0419.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0418.mp4\n",
      "/workspace/qalign_features/val/0418.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0417.mp4\n",
      "/workspace/qalign_features/val/0417.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0416.mp4\n",
      "/workspace/qalign_features/val/0416.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0415.mp4\n",
      "/workspace/qalign_features/val/0415.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0414.mp4\n",
      "/workspace/qalign_features/val/0414.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0413.mp4\n",
      "/workspace/qalign_features/val/0413.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0412.mp4\n",
      "/workspace/qalign_features/val/0412.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0411.mp4\n",
      "/workspace/qalign_features/val/0411.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0410.mp4\n",
      "/workspace/qalign_features/val/0410.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0409.mp4\n",
      "/workspace/qalign_features/val/0409.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0408.mp4\n",
      "/workspace/qalign_features/val/0408.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0407.mp4\n",
      "/workspace/qalign_features/val/0407.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0406.mp4\n",
      "/workspace/qalign_features/val/0406.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0405.mp4\n",
      "/workspace/qalign_features/val/0405.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0404.mp4\n",
      "/workspace/qalign_features/val/0404.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0403.mp4\n",
      "/workspace/qalign_features/val/0403.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0402.mp4\n",
      "/workspace/qalign_features/val/0402.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0401.mp4\n",
      "/workspace/qalign_features/val/0401.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0400.mp4\n",
      "/workspace/qalign_features/val/0400.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0399.mp4\n",
      "/workspace/qalign_features/val/0399.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0398.mp4\n",
      "/workspace/qalign_features/val/0398.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0397.mp4\n",
      "/workspace/qalign_features/val/0397.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0396.mp4\n",
      "/workspace/qalign_features/val/0396.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0395.mp4\n",
      "/workspace/qalign_features/val/0395.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0394.mp4\n",
      "/workspace/qalign_features/val/0394.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0393.mp4\n",
      "/workspace/qalign_features/val/0393.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0392.mp4\n",
      "/workspace/qalign_features/val/0392.npy\n",
      "torch.Size([3, 2, 4096])\n",
      "(3, 4096)\n",
      "/workspace/val/val/0391.mp4\n",
      "/workspace/qalign_features/val/0391.npy\n",
      "torch.Size([3, 2, 4096])\n",
      "(3, 4096)\n",
      "/workspace/val/val/0390.mp4\n",
      "/workspace/qalign_features/val/0390.npy\n",
      "torch.Size([3, 2, 4096])\n",
      "(3, 4096)\n",
      "/workspace/val/val/0389.mp4\n",
      "/workspace/qalign_features/val/0389.npy\n",
      "torch.Size([3, 2, 4096])\n",
      "(3, 4096)\n",
      "/workspace/val/val/0388.mp4\n",
      "/workspace/qalign_features/val/0388.npy\n",
      "torch.Size([3, 2, 4096])\n",
      "(3, 4096)\n",
      "/workspace/val/val/0387.mp4\n",
      "/workspace/qalign_features/val/0387.npy\n",
      "torch.Size([3, 2, 4096])\n",
      "(3, 4096)\n",
      "/workspace/val/val/0386.mp4\n",
      "/workspace/qalign_features/val/0386.npy\n",
      "torch.Size([3, 2, 4096])\n",
      "(3, 4096)\n",
      "/workspace/val/val/0385.mp4\n",
      "/workspace/qalign_features/val/0385.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0384.mp4\n",
      "/workspace/qalign_features/val/0384.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0383.mp4\n",
      "/workspace/qalign_features/val/0383.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0382.mp4\n",
      "/workspace/qalign_features/val/0382.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0381.mp4\n",
      "/workspace/qalign_features/val/0381.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0380.mp4\n",
      "/workspace/qalign_features/val/0380.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0379.mp4\n",
      "/workspace/qalign_features/val/0379.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0378.mp4\n",
      "/workspace/qalign_features/val/0378.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0377.mp4\n",
      "/workspace/qalign_features/val/0377.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0376.mp4\n",
      "/workspace/qalign_features/val/0376.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0375.mp4\n",
      "/workspace/qalign_features/val/0375.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0374.mp4\n",
      "/workspace/qalign_features/val/0374.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0373.mp4\n",
      "/workspace/qalign_features/val/0373.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0372.mp4\n",
      "/workspace/qalign_features/val/0372.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0371.mp4\n",
      "/workspace/qalign_features/val/0371.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0370.mp4\n",
      "/workspace/qalign_features/val/0370.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0369.mp4\n",
      "/workspace/qalign_features/val/0369.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0368.mp4\n",
      "/workspace/qalign_features/val/0368.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0367.mp4\n",
      "/workspace/qalign_features/val/0367.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0366.mp4\n",
      "/workspace/qalign_features/val/0366.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0365.mp4\n",
      "/workspace/qalign_features/val/0365.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0364.mp4\n",
      "/workspace/qalign_features/val/0364.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0363.mp4\n",
      "/workspace/qalign_features/val/0363.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0362.mp4\n",
      "/workspace/qalign_features/val/0362.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0361.mp4\n",
      "/workspace/qalign_features/val/0361.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0360.mp4\n",
      "/workspace/qalign_features/val/0360.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0359.mp4\n",
      "/workspace/qalign_features/val/0359.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0358.mp4\n",
      "/workspace/qalign_features/val/0358.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0357.mp4\n",
      "/workspace/qalign_features/val/0357.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0356.mp4\n",
      "/workspace/qalign_features/val/0356.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0355.mp4\n",
      "/workspace/qalign_features/val/0355.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0354.mp4\n",
      "/workspace/qalign_features/val/0354.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0353.mp4\n",
      "/workspace/qalign_features/val/0353.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0352.mp4\n",
      "/workspace/qalign_features/val/0352.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0351.mp4\n",
      "/workspace/qalign_features/val/0351.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0350.mp4\n",
      "/workspace/qalign_features/val/0350.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0349.mp4\n",
      "/workspace/qalign_features/val/0349.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0348.mp4\n",
      "/workspace/qalign_features/val/0348.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0347.mp4\n",
      "/workspace/qalign_features/val/0347.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0346.mp4\n",
      "/workspace/qalign_features/val/0346.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0345.mp4\n",
      "/workspace/qalign_features/val/0345.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0344.mp4\n",
      "/workspace/qalign_features/val/0344.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0343.mp4\n",
      "/workspace/qalign_features/val/0343.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0342.mp4\n",
      "/workspace/qalign_features/val/0342.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0341.mp4\n",
      "/workspace/qalign_features/val/0341.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0340.mp4\n",
      "/workspace/qalign_features/val/0340.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0339.mp4\n",
      "/workspace/qalign_features/val/0339.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0338.mp4\n",
      "/workspace/qalign_features/val/0338.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0337.mp4\n",
      "/workspace/qalign_features/val/0337.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0336.mp4\n",
      "/workspace/qalign_features/val/0336.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0335.mp4\n",
      "/workspace/qalign_features/val/0335.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0334.mp4\n",
      "/workspace/qalign_features/val/0334.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0333.mp4\n",
      "/workspace/qalign_features/val/0333.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0332.mp4\n",
      "/workspace/qalign_features/val/0332.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0331.mp4\n",
      "/workspace/qalign_features/val/0331.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0330.mp4\n",
      "/workspace/qalign_features/val/0330.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0329.mp4\n",
      "/workspace/qalign_features/val/0329.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0328.mp4\n",
      "/workspace/qalign_features/val/0328.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0327.mp4\n",
      "/workspace/qalign_features/val/0327.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0326.mp4\n",
      "/workspace/qalign_features/val/0326.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0325.mp4\n",
      "/workspace/qalign_features/val/0325.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0324.mp4\n",
      "/workspace/qalign_features/val/0324.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0323.mp4\n",
      "/workspace/qalign_features/val/0323.npy\n",
      "torch.Size([1, 2, 4096])\n",
      "(1, 4096)\n",
      "/workspace/val/val/0322.mp4\n",
      "/workspace/qalign_features/val/0322.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0321.mp4\n",
      "/workspace/qalign_features/val/0321.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0320.mp4\n",
      "/workspace/qalign_features/val/0320.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0319.mp4\n",
      "/workspace/qalign_features/val/0319.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0318.mp4\n",
      "/workspace/qalign_features/val/0318.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0317.mp4\n",
      "/workspace/qalign_features/val/0317.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0316.mp4\n",
      "/workspace/qalign_features/val/0316.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0315.mp4\n",
      "/workspace/qalign_features/val/0315.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0314.mp4\n",
      "/workspace/qalign_features/val/0314.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0313.mp4\n",
      "/workspace/qalign_features/val/0313.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0312.mp4\n",
      "/workspace/qalign_features/val/0312.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0311.mp4\n",
      "/workspace/qalign_features/val/0311.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0310.mp4\n",
      "/workspace/qalign_features/val/0310.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0309.mp4\n",
      "/workspace/qalign_features/val/0309.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0308.mp4\n",
      "/workspace/qalign_features/val/0308.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0307.mp4\n",
      "/workspace/qalign_features/val/0307.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0306.mp4\n",
      "/workspace/qalign_features/val/0306.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0305.mp4\n",
      "/workspace/qalign_features/val/0305.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0304.mp4\n",
      "/workspace/qalign_features/val/0304.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0303.mp4\n",
      "/workspace/qalign_features/val/0303.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0302.mp4\n",
      "/workspace/qalign_features/val/0302.npy\n",
      "torch.Size([9, 2, 4096])\n",
      "(9, 4096)\n",
      "/workspace/val/val/0301.mp4\n",
      "/workspace/qalign_features/val/0301.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0300.mp4\n",
      "/workspace/qalign_features/val/0300.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0299.mp4\n",
      "/workspace/qalign_features/val/0299.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0298.mp4\n",
      "/workspace/qalign_features/val/0298.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0297.mp4\n",
      "/workspace/qalign_features/val/0297.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0296.mp4\n",
      "/workspace/qalign_features/val/0296.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0295.mp4\n",
      "/workspace/qalign_features/val/0295.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0294.mp4\n",
      "/workspace/qalign_features/val/0294.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0293.mp4\n",
      "/workspace/qalign_features/val/0293.npy\n",
      "torch.Size([8, 2, 4096])\n",
      "(8, 4096)\n",
      "/workspace/val/val/0292.mp4\n",
      "/workspace/qalign_features/val/0292.npy\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "path = '/workspace/val/val/'\n",
    "videos = os.listdir(path)\n",
    "for video in videos:\n",
    "    video_path = os.path.join(path,video)\n",
    "    save_path = '/workspace/qalign_features/val/' + video.split('.')[0] + '.npy'\n",
    "    print(video_path)\n",
    "    print(save_path)\n",
    "    get_hidden_states(video_path,save_path)\n",
    "\n",
    "path = '/workspace/train/train/'\n",
    "videos = os.listdir(path)\n",
    "for video in videos:\n",
    "    video_path = os.path.join(path,video)\n",
    "    save_path = '/workspace/qalign_features/train/' + video.split('.')[0] + '.npy'\n",
    "    print(video_path)\n",
    "    print(save_path)\n",
    "    get_hidden_states(video_path,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4c88a-0976-4eda-a513-4074715d1584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
